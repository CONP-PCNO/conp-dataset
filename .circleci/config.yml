version: 2

aliases:
  - &workdir /src/conp-dataset

  - &docker
    - image: mathdugre/conp-dataset:2.0

  - &restore_python_cache
    restore_cache:
      name: Restore conda cache
      keys:
        - v1.1-conda-{{ .Branch }}
        - v1.1-conda-

  - &restore_dataset
    restore_cache:
      name: Restore dataset installation
      keys:
        - v1.1-dataset-{{ .Branch }}
        - v1.1-dataset-

jobs:
  build:
    working_directory: *workdir
    docker: *docker

    steps:
      - checkout:
          path: *workdir
      - *restore_python_cache
      - *restore_dataset
      - run:
          name: Install dependencies
          command: |
            pip install -r requirements.txt 
            pip install -r tests/requirements.txt
            pip install -r scripts/dats_validator/requirements.txt
      - save_cache:
          name: Save conda cache
          paths:
            - /src/miniconda/bin
            - /src/miniconda/lib/python3.7/site-packages
          key: v1.1-conda-{{ .Branch }}
      - run:
          name: Install dataset
          command: datalad install -r .
      - save_cache:
          name: Save dataset installation
          paths:
            - /src/conp-dataset
          key: v1.1-dataset-{{ .Branch }}

  test:
    working_directory: *workdir
    docker: *docker
    parallelism: 2

    steps:
      - *restore_python_cache
      - *restore_dataset
      - run:
          name: Run tests
          no_output_timeout: 10m
          command: |
            python tests/create_tests.py
            mkdir test-results
            circleci tests glob tests/test_* | circleci tests split --split-by=timings > /tmp/tests-to-run 
            PYTHONPATH=$PWD pytest -n 2 --junitxml=test-results/junit.xml -v -rfEs $(cat /tmp/tests-to-run)
      - store_test_results:
          path: test-results

workflows:
  version: 2
  build_and_test:
    jobs:
      - build
      - test:
          requires:
            - build
