version: 2
jobs:
  build:
    working_directory: ~/conp-dataset
    docker:
      - image: mathdugre/conp-dataset
    parallelism: 2

    steps:
      - checkout
      - restore_cache:
          keys:
            - conda-dependencies-v0.0.2-{{ .Branch }}
            - conda-dependencies-v0.0.2-
      - run:
          name: Install dependencies
          command: |
            pip install -r requirements.txt 
            pip install -r tests/requirements.txt
            pip install -r scripts/dats_validator/requirements.txt
            pip freeze
      - save_cache:
          paths:
            - /opt/conda/bin
            - /opt/conda/lib/python3.7/site-packages/
          key: conda-dependencies-v0.0.2-{{ .Branch }}

      - restore_cache:
          keys:
            - conp-dataset-v0.0.1-{{ .Branch }}
            - conp-dataset-v0.0.1-
      - run:
          name: Install dataset
          command: datalad install -r .
      - save_cache:
          paths:
            - ~/conp-dataset
          key: conp-dataset-v0.0.1-{{ .Branch }}

      - run:
          name: Create test file for each datasets
          command: python tests/create_tests.py
      - run:
          name: Run test suite
          no_output_timeout: 10m
          command: |
            mkdir test-results
            export PYTHONPATH=$PWD
            circleci tests glob tests/test_* | circleci tests split --split-by=timings > /tmp/tests-to-run 
            pytest --junitxml=test-results/junit.xml -v -rfEs $(cat /tmp/tests-to-run)
      - store_test_results:
          path: test-results
